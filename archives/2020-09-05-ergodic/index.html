<!DOCTYPE html>
<html lang="en">
<head>
  <title></title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" href="/assets/css/main.css"/>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans&family=Roboto+Mono:wght@400;500;600&display=swap" rel="stylesheet">
  <link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300&display=swap" rel="stylesheet">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/assets/js/jquery-2.1.1.min.js"></script>
<script src="/assets/js/functions.js" type="text/javascript"></script>
</head>
<body style="margin: 0;">


	<div class="post_wrapper">
	<div class="backbutton">
		<a href="/index.html"><img src = "/assets/img/house.svg"/> Home</a>
		<a href="/allposts/index.html"><img src = "/assets/img/mailbox.svg"/> All posts</a>
	</div>
	
	<div class="post_inner">
		<div class="main_info">
			<h1>Measure Theory, Ergodicity, and Entropy</h1>
			
			<h3>The layman's introduction to ergodic theory and entropy.</h3>

			<p></p>
		</div>
		<div class="main_post">
			<h3>Sugary lattes</h3>
<p>What do baristas and bakers have in common? They both mix things. Baristas mix water or milk with espressos, and bakers (sometimes) mix cinnamon or chocolate chips into dough.</p>

<h3>What is size?</h3>

<p>Measure theory defines the concept of size. Length, area, volume, and even counting are all used to measure the "size" of an object, so mathematicians refer to these measurement concepts as measures. Unfortunately the overarching commonalities between these measures, or the essence of a measure, is difficult to define rigorously. For our purposes, going into the nitty-gritty of what makes a measure isn't important, but here are the three defining qualities of a measure.
<ol>
	<li>The measure of nothing (null-ness, emptiness, etc.) is 0.</li>
	<li>The measure of an object is always greater than or equal to 0.</li>
	<li>The measure of a whole is equal to the sum of its nonoverlapping parts' measures.</li>
</ol>

To give an example, <a href="https://en.wikipedia.org/wiki/Counting_measure">counting</a> satisfies all the measure conditions. A sock drawer always contains no socks or a positive number of socks, unless counting negative numbers of things is possible -  if you can, shoot me a message because I like magic. If not, counting surely satisfies the first and second qualities. For the last quality, notice the obvious fact that two separate sock drawers, each with 10 socks, hold a total of 20 socks.</p>
<p>As with all things math or philosophy related, we have to define the concept of zero/nothingness. Intuitively, something with zero length is nothing at all; it has no dimension and doesn't exist. Measure theory says different. Take a line segment that <a href="https://en.wikipedia.org/wiki/Unit_interval#:~:text=In%20mathematics%2C%20the%20unit%20interval,I%20(capital%20letter%20I).">extends from 0 to 1</a>, which has a length/measure of 1. Removing 0 and 1 from this line doesn't change the line's length at all, so the points 0 and 1 must both have measures of 0. This means that in the measure theoretical sense, it's possible for non-nothing objects to have a measure of zero - a "zero-ness", if you will. </p>

<img src="/assets/data/entropy/unitdenial.jpg">

<p>This loose definition of zero means that measure theory can provide a rigorous basis for our intuitions concerning "almost-ness". Statements that make intuitive sense, such as "almost every pug has a squiggly tail" or "almost everyone has hair", now have a rigorous basis. To say that a property holds for <i>almost</i> everything means that the measure of those things that <b>don't</b> satisfy the property is zero. For example, if one tiny drop of green food coloring is dropped into a glass of water, then since the measure of the tiny green drop is practically 0, <i>almost</i> every particle in the glass is water.</p>


<h3>Ergodic farms</h3>
<p>A process is ergodic if it mixes the particles of a system "well". Concretely speaking, given any significant sub-region of a system, an ergodic process mixes the system's particles in such a way that they travel in and out of the selected region. To picture this, imagine cows in a fenced cow field. An ergodic farm has no patches of grass that contain the same cows for all eternity; in contrast, an non-ergodic farm has at least one patch where the same cows dwell forever. In the image below, the <span style="color: red;">red</span> indicates individual cow movement, and the <span style="color: green;">green</span> marks an arbitrary patch of grass in the ergodic case and a specific patch in the other.</p>

<img src="/assets/data/entropy/entropycow.jpg">

See how an ergodic process moves the cows around so that no cows stay in the same region forever? Likewise, a non-ergodic process has a specific patch where the cows inside it <a href="https://giphy.com/gifs/GYeLcrWi5DMqs/html5">never leave</a>, and the cows outside never enter. 

<h3>Dartboards and Time Machines</h3>
Imagine a dartboard, with a total area of 1, where each region's score is inversely proportional to its area; the bigger the region, the less it's worth. Of course, we expect to hit the larger regions more than the smaller regions, which means that in the rare occasion that we do hit a small region, we gain a large number of points. But we want to know how many points that we can expect to score. This is the expected value, and in this case, it coincides with the definition of entropy. 

<img src="/assets/data/entropy/dartboard1.jpeg">

Now imagine that in the weird metaphorical world that we are in, the dartboard regions change every second. 

<img src="/assets/data/entropy/dartboard2.jpeg">

<h4>Further Reading</h4>
<ol>
	<li><p>For the more mathematically inclined, see my relatively self-contained <a href="/assets/data/ergodic_entropy.pdf">exposition on ergodic theory and entropy</a>; it assumes knowledge of Lebesgue integration, and introduces various notions of entropy such as measure-theoretical entropy and the Lyapunov exponent.</p>
	</li>
	<li><p>Cesar Ernesto Silva's <a href="https://www.amazon.com/Invitation-Ergodic-Student-Mathematical-Library/dp/0821844202">Invitation to Ergodic Theory</a> is a self-contained measure theory/ergodic theory text. This is what I used to study dynamics during the summer - it's awesome.</p></li>
</ol>






		</div>
	</div>
</div>

<div class="space" style="height: 50px"></div>

</body>
</html>