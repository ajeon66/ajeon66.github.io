<!DOCTYPE html>
<html lang="en">
<head>
  <title></title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400;500&family=Quicksand:wght@300;400;500&family=Roboto+Slab:wght@100;300;400;700&display=swap" rel="stylesheet">

  
  	





  
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<div class="posthead">
	<div class="postheadouter">
		<div class="postheadinner">
			<nav>
				<a href="/posts/index.html">posts.</a>
				<a href="/index.html">home.</a>
			</nav>

			<div class="posthead">
				<h1>I Read Papers: 6/1/2020 ~ 6/5/2020</h1>
				<p>This week, I read about information security risks due to contact tracing methods, and backdoor attacks against NLP models.</p>
			</div>
		</div>
	</div>
</div>

<div class="posttextouter">
	<div class="posttextinner">
		<div class="posttext">
			<p>Relevant papers: <a href="https://arxiv.org/abs/2006.00529">[1]</a>, <a href="https://arxiv.org/abs/2006.01043">[2].</a></p>

<h2><a href="https://arxiv.org/abs/2006.00529">Information Security + Contact Tracing: COVID-19.</a></h2>
<h3>General Information</h3>
<p>Many countires have implemented mobile phone application services to trace the infected's whereabouts.</p>

<h2><a href="https://arxiv.org/abs/2006.01043">BadNL: Backdoor Attacks Against NLP Models</a></h2>
<h3>General Information</h3>
<p>Chen et al. proposes a backdoor attack against sentiment analysis models with the goal of inducing the model to mislabel inputs by poisoning the inputs with triggers for desired labels. One of the applications of this attack, as given in the paper, is the misclassification of hate speech; a user tweets hateful speech embedded with triggers, causing the backdoored Twitter NLP model to mislabel the tweet as “appropriate”, thus allowing the tweet to spread among the masses. </p>

<p>A backdoor attack’s success against NLP models is measured by three conditions - how much of the original sentiment is retained, how often the inputs poisoned with “trigger labels” result in the desired labelling, and how much of the original model accuracy on clean inputs is retained. The first condition concerns the goal of the attack. Going back to the malicious tweet example, suppose the first condition is not met. The hate speech would no longer be hate speech, and the model would thus correctly label it as “appropriate”. Therefore, the added triggers should be significant enough to cause mislabelling, but subtle enough not to alter the sentiment. Next, the second condition measures the attack’s effective damage. Finally, the third condition affects the attack’s detectability. 
</p>

<p>The attack is designed as follows. Let \(D_{clean} \) be the clean data, \(D_{dirty} \) the poisoned data, and \(T \) the trigger space. Define the backdoor function as \( A: D_{clean} \times T \to D_{dirty} \). \(A \) constructs \(D_{dirty} \) by adding the elements of \(T \) to a subset of \( D_{clean} \). As shown in the paper, character, word, and sentence triggers give three variations of the backdoor function, which we will call \( A_{char}, A_{word},\) and \(A_{sent} \) respectively. In all variations, the trigger \( t \in T \) is introduced to the start, middle, or end of the input text i.e. the position of the poisoned word/phrase.</p>

<p>Character-level triggers aim to leverage typographical errors to trigger the desired backdoor behavior. To give an example, consider $$A_{char}(\text{I enjoyed this film}, \text{ fill}) = \text{I enjoyed this fill}.$$ If the above sentence is the last sentence of an input text, then this is an end-manipulation. Likewise, a start-manipulation is when the first word of an input text is altered.</p>

<p>Word-level triggers insert a particular word into the input, based on the assumption that repeated use of a trigger will induce the target label: $$A_{word}(\text{I enjoyed this film, potion}) = \text{I enjoyed this film potion.}$$</p>

<p>Finally, sentence-level triggers convert the current verb tense to an obscure tense to induce the target label: $$A_{sent}(\text{I enjoyed this film, will have been enjoying}) = \text{I will have been enjoying this film}.$$</p>

<p>Based on the three metrics of a successful backdoor attack, word-level triggers come first, with the sentence and character-level triggers coming second and third, respectively. However, in terms of visibility/detectability, it is obvious that character-level insertions are the most invisible, as typographical errors are extremely prevalent and thus considered trivial; on the other hand, sentence and word-level triggers are fairly noticeable. As always is the case with cybersecurity, there exists a tradeoff between visibility and effectiveness.</p>

<h3>Thoughts</h3>
<p>I had no idea attacks against machine learning models existed. However, with the growing interest in machine learning, these attacks pose serious threats, particularly towards businesses whose primary product is a machine learning solution. How would you protect against something like. Furthermore, how would one even gain access to, say, Twitter's sentiment analysis model in the first place? Furthermore, can these attacks have more ethical/moral consequences? I believe that the bulk of cybersecurity issues are concerned with protecting privacy. How can these backdoor attacks affect </p>
			<div class="taglink">
	    		
	    			<a href="/tag/ireadpapers">#ireadpapers</a>
	    		
	    	</div>
		</div>
	</div>
</div>

<footer>
	<div class="footerinner">
		<div class="foot">
			<p>Designed and created by Alex Jeon using Jekyll. Hosted by Github Pages.</p>
		</div>
	</div>
</footer>

<script src="/assets/js/jquery-2.1.1.min.js"></script>
<script src="/assets/js/functions.js" type="text/javascript"></script>
</body>
</html>