<!DOCTYPE html>
<html lang="en">
<head>
  <title></title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/ajeon66.github.io/assets/css/main.css" />
  <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400;500&family=Quicksand:wght@300;400;500&family=Roboto+Slab:wght@100;300;400;700&display=swap" rel="stylesheet">

  
  	





  
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<div class="posthead">
	<div class="postheadouter">
		<div class="postheadinner">
			<nav>
				<a href="/ajeon66.github.io/posts/index.html">posts.</a>
				<a href="/ajeon66.github.io/index.html">home.</a>
			</nav>

			<div class="posthead">
				<h1>I Read Papers: 6/1/2020 ~ 6/5/2020</h1>
				<p>This week, I read about backdoor attacks against NLP models.</p>
			</div>
		</div>
	</div>
</div>

<div class="posttextouter">
	<div class="posttextinner">
		<div class="posttext">
			<p>Relevant paper(s): <a href="https://arxiv.org/abs/2006.01043">[1].</a></p>
<h2><a href="https://arxiv.org/abs/2006.01043">BadNL: Backdoor Attacks Against NLP Models</a></h2>
<h3>General Information</h3>
<p>Chen et al. proposes a backdoor attack against sentiment analysis models with the goal of inducing the model to mislabel inputs by poisoning the inputs with triggers for desired labels. One of the applications of this attack, as given in the paper, is the misclassification of hate speech; a user tweets hateful speech embedded with triggers, causing the backdoored Twitter NLP model to mislabel the tweet as “appropriate”, thus allowing the tweet to spread among the masses. </p>

<p>A backdoor attack’s success against NLP models is measured by three conditions - how much of the original sentiment is retained, how often the inputs poisoned with “trigger labels” result in the desired labelling, and how much of the original model accuracy on clean inputs is retained. The first condition concerns the goal of the attack. Going back to the malicious tweet example, suppose the first condition is not met. The hate speech would no longer be hate speech, and the model would thus correctly label it as “appropriate”. Therefore, the added triggers should be significant enough to cause mislabelling, but subtle enough not to alter the sentiment. Next, the second condition measures the attack’s effective damage. Finally, the third condition affects the attack’s detectability. 
</p>

<p>The attack is designed as follows. Let \(D_{clean} \) be the clean data, \(D_{dirty} \) the poisoned data, and \(T \) the trigger space. Define the backdoor function as \( A: D_{clean} \times T \to D_{dirty} \). \(A \) constructs \(D_{dirty} \) by adding the elements of \(T \) to a subset of \( D_{clean} \). As shown in the paper, character, word, and sentence triggers give three variations of the backdoor function, which we will call \( A_{char}, A_{word},\) and \(A_{sent} \) respectively. In all variations, the trigger \( t \in T \) is introduced to the start, middle, or end of the input text i.e. the position of the poisoned word/phrase.</p>

<p>Character-level triggers aim to leverage typographical errors to trigger the desired backdoor behavior. To give an example, consider $$A_{char}(\text{I enjoyed this film}, \text{ fill}) = \text{I enjoyed this fill}.$$ If the above sentence is the last sentence of an input text, then this is an end-manipulation. Likewise, a start-manipulation is when the first word of an input text is altered.</p>

<p>Word-level triggers insert a particular word into the input, based on the assumption that repeated use of a trigger will induce the target label: $$A_{word}(\text{I enjoyed this film, potion}) = \text{I enjoyed this film potion.}$$</p>

<p>Finally, sentence-level triggers convert the current verb tense to an obscure tense to induce the target label: $$A_{sent}(\text{I enjoyed this film, will have been enjoying}) = \text{I will have been enjoying this film}.$$</p>

<p>Based on the three metrics of a successful backdoor attack, word-level triggers come first, with the sentence and character-level triggers coming second and third, respectively. However, in terms of visibility/detectability, it is obvious that character-level insertions are the most invisible, as typographical errors are extremely prevalent and thus considered trivial; on the other hand, sentence and word-level triggers are fairly noticeable. As always is the case with cybersecurity, there exists a tradeoff between visibility and effectiveness.</p>

<h3>
	Questions and Thoughts
</h3>
<ol>
	<li>Where exactly the data tampering comes in is unclear. From context, the input text seems to be directly tampered with, so what the model sees is what everyone else sees. However, is it not possible to leave \(D_{clean} \) as is, and submit only \(D_{dirty} \) to the model? I may be missing something here.</li>
	<li>How are word-level insertions made? The insertion of just one word (negation, most notably) could reverse the sentiment, so another layer of caution must be exercised due to semantic restraints. Is it a loop (brute force)? Is it a Markov chain? Character and sentence-level triggers seem straightforward. The former consists of replacing the targeted input with a similar word. With the latter, changing tenses does not pose a huge linguistic problem, as most sentences in English follow a subject-verb structure. However, how words can be inserted effectively is a little ambiguous.</li>
	<li>Depending on the language, could the three backdoor functions' effectivenesses change? Getting into linguistics terrority here, but what would a backdoor function that attacks a Chinese sentiment analysis model look like? Does romanticization of Chinese through pinyin change anything?</li>
	<li>A long time ago, I read about a chatbot that started spewing obscenities and other.. nasty things because the chatbot used its interactions with users as training data. However, to gain access to, say, Twitter's sentiment training data is not as simple as typing obscenities at a chatbot. How would one gain access to the training data and the model in the first place?</li>
	<li>Socially and morally speaking, in what ways does machine learning affect us? If I were a hacker that had access to a \(D_{clean} \), I would steal the data rather than manipulate the model that the data affects, because data is more valuable than said data's interpretation. In the future, when machine learning contributes to and holds a more decisive and authoritative position, this backdoor implementation will pose a severe threat.</li>
</ol>

			<div class="taglink">
	    		
	    			<a href="/ajeon66.github.io/tag/ireadpapers">#ireadpapers</a>
	    		
	    	</div>
		</div>
	</div>
</div>

<footer>
	<div class="footerinner">
		<div class="foot">
			<p>Designed and created by Alex J. using Jekyll. Hosted by Github Pages.</p>
		</div>
	</div>
</footer>

<script src="/ajeon66.github.io/assets/js/jquery-2.1.1.min.js"></script>
<script src="/ajeon66.github.io/assets/js/functions.js" type="text/javascript"></script>
</body>
</html>