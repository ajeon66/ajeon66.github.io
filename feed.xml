<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:3000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:3000/" rel="alternate" type="text/html" /><updated>2021-07-04T16:55:17+09:00</updated><id>http://localhost:3000/feed.xml</id><entry><title type="html">Today in News</title><link href="http://localhost:3000/2021/06/30/news/" rel="alternate" type="text/html" title="Today in News" /><published>2021-06-30T00:00:00+09:00</published><updated>2021-06-30T00:00:00+09:00</updated><id>http://localhost:3000/2021/06/30/news</id><content type="html" xml:base="http://localhost:3000/2021/06/30/news/">&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;Louis Pasteur said &quot;where observation is concerned, chance favors only the prepared mind.&quot; For me, there are three components to intellectual growth: observation, intake, and reflection. Observation is by far the component that I pay the least attention to, and this page is an attempt to give more effort to observation.&lt;/p&gt;

&lt;p&gt;In other words, I&apos;m just trying to trick myself into not being a theoretical-head-in-the-clouds geek. Mind you, the news articles listed here are not the most important or interesting, but more a reflection of what I am able to address.&lt;/p&gt;

&lt;h3&gt;Sunday, July 3, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] I think this highlights a number of interesting things. 
		&lt;ol&gt;
			&lt;li&gt;Power graphs (i.e. the distribution of power) are important, and they are not always equivalent to the distribution of agents.&lt;/li&gt;
			&lt;li&gt;We sometimes forget, but software - and technology in general - is hard. The costs of implementing in-house systems for small/medium-sized businesses that are not in the tech space are far greater than just getting a third-party to take care of everything.&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/a8e7c9a2-5819-424f-b087-c6f2e8f0c7a1&quot;&gt;Hackers&lt;/a&gt; hit Kaseya, an IT software provider, to spread ransomware. Consequently, many of Sweden&apos;s Coop grocery chain stores, which depend on Kaseya for their cash register and self-checkout systems, remained closed. This attack suggests the dependence of many small/medium-sized companies on centralized third parties for their IT service needs. [1]&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/03/digital-violence-nso-group-spyware/&quot;&gt;Digital Forensics&lt;/a&gt; presents an &lt;a href=&quot;https://www.digitalviolence.org/#/&quot;&gt;interactive timeline&lt;/a&gt; of the state terror caused by the NSO Group&apos;s spyware &lt;i&gt;Pegasus&lt;/i&gt;. The spyware is allegedly capable of activating personal smartphone cameras and microphones, hence leveraging the widespread use of smartphones to gain intelligence. &lt;div class=&quot;marginnote&quot;&gt;[2] I&apos;m working on a thesis that as things become more niche, the need to standardize and homogenize increases also. Pegasus seems to be exploiting this thesis, if true.&lt;/div&gt; Digital Forensics developed digitalviolence to counter the use of spyware like Pegasus towards questionable, unethical purposes. [2]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/legal/litigation/case-watch-toxic-commercial-weedkiller-fuels-new-mass-tort-2021-07-02/&quot;&gt;Herbicide&lt;/a&gt; suggested to be linked to Parkinson&apos;s disease could fuel a new mass tort. Paraquat is a known toxic herbicide marketed by Syngenta and previously sold by USA&apos;s Chevron. A tort may arise from data showing that chronic exposure to paraquat may be correlated with Parkinson&apos;s disease.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Saturday, July 3, 2021&lt;/h3&gt;

&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://twitter.us18.list-manage.com/track/click?u=92fd2e3ec7962cda008f0732a&amp;id=23e3ae9eb6&amp;e=1c03e42b50&quot;&gt;Cars&lt;/a&gt; are back in fashion. [1] A primary reason for this resurgence in car ownership is the pandemic, which discouraged the use of public transportation (due to fears of viral transmission). Used car prices and waiting times for driving tests have risen. An interesting observation is that, at least in America, the total distance travelled by car is not even at pre-pandemic levels. I would guess that this discrepancy emerges from the prevalence of remote work and other stay-at-home policies.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/energy/opec-seeks-oil-policy-consensus-uae-demands-changes-2021-07-02/&quot;&gt;UAE&lt;/a&gt; wants OPEC+ to let it raise oil output. OPEC+ is looking to increase oil supply in the market to ease a surge in oil prices. However, UAE have rejected this new deal on the grounds that their maximum production capacity has increased since 2018, so the proposed deal will cut into their production revenues more than it will for Russia and Saudi Arabia.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] I really wonder how our ancestors would react if they saw 1. people getting information from these massively vast social graphs and 2. the mediators of these social graphs policing the information.&lt;/div&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[4] It&apos;d be cool to build a GAN with the Twitter misinformation labeller as the adversary.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/07/01/twitter-colorful-misinformation-labels/&quot;&gt;Twitter&lt;/a&gt; rolls out misinformation labels. The labels color-code misleading information, and include a link that takes you to a curated landing page with verified information. The role of social networks in the spread of information (true or un-true alike) is interesting. Social graphs have always been a source of information, but the constant verification of information with social origins is the fascinating bit. [3] I suspect a lot of Americans will call this a rights infringement, but that&apos;s a separate conversation altogether. [4]&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.wsj.com/articles/china-india-move-tens-of-thousands-of-troops-to-the-border-in-largest-buildup-in-decades-11625218201?cx_testId=200&amp;cx_testVariant=cx_10&amp;cx_artPos=0#cxrecs_s&quot;&gt;Tensions&lt;/a&gt; between China and India rise. Both sides have been fortifying their positions, continuing the escalating narrative since the June 2020 Galwan Valley clash.&lt;/li&gt;

&lt;/ol&gt;


&lt;h3&gt;Friday, July 2, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] From John Kemp&apos;s energy email blast.&lt;/div&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/sustainable-business/us-natgas-companies-put-hydrogen-test-2021-07-01/&quot;&gt;Natural gas suppliers&lt;/a&gt; [1] experiment with hydrogen blending. Hydrogen blending is a method of reducing carbon emissions from natural gas by mixing hydrogen into natural gas pipelines. Apparently, there is such a thing as &lt;a href=&quot;https://en.wikipedia.org/wiki/Green_hydrogen&quot;&gt;green hydrogen&lt;/a&gt;, which is hydrogen produced from low-carbon sources (as opposed to the usual fossil fuel sources). The high cost of green hydrogen production seems to be the barrier.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] From John Kemp&apos;s energy email blast.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.eia.gov/todayinenergy/detail.php?id=48536&quot;&gt;California&lt;/a&gt; [2] and other major regions are at risk of energy shortages this summer. Peak energy demand may increase given the heat wave. California is most at risk because it relies on energy imports, and solar energy output falls in the late afternoon.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[3] A similar mechanism to why increasing interest rates theoretically increases the home currency&apos;s value?&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-07-01/global-tax-overhaul-wins-broad-consensus-but-obstacles-remain&quot;&gt;130 countries&lt;/a&gt; back a plan for setting the minimum global corporate tax rate at 15%. The minimum tax rate is meant to discourage multinational corporations from shifting their money to lower-tax countries.[3] Lower-tax countries such as Ireland have benefitted from the influx of investments from multi-national companies, so a final agreement could have major repurcussions.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[4] I don&apos;t know anything about the housing market, so this is an excuse for me to learn more about it.&lt;/div&gt;
	
	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/features/2021-07-01/world-s-fastest-property-price-surge-since-financial-crisis-sparks-bidding-wars&quot;&gt;Property valuations&lt;/a&gt; are soaring globally. I wonder if the pandemic has made people more selective about their living spaces.[4]&lt;/li&gt;

	

	
&lt;/ol&gt;

&lt;h3&gt;Thursday, July 1, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;div class=&quot;marginnote&quot;&gt;[1] For one, the presenter doesn&apos;t gesticulate nor have any facial expressions.&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/06/30/look-out-language-teachers-a-synthetic-human-could-be-about-to-take-your-job/&quot;&gt;Hour One&lt;/a&gt;, a startup that creates photo-real presenters, signs a deal with Berlitz. COVID has led to greater acceptance of technological solutions for traditionally offline things, but I&apos;m curious how this turns out. I do think this has the most application in language learning, since being able to see a mouth sounding out words can be crucial for learning pronunciation. However, I&apos;m doubtful that putting a face to text is really more engaging.[1] Also text has the greatest signal density, so by using this presenter, the signal density goes down significantly.&lt;/li&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[2] &quot;It&apos;s called the little bighorn. That&apos;s smart, Mark.&quot;&lt;/div&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/df4f6e4c-9ee5-47ff-aa8f-8c4c328a1f39&quot;&gt;Zipline&lt;/a&gt;, a drone delivery startup, raises $250 million. Zipline first attacked the African space by making strides in medical supply lines, using drones to deliver vaccines and medicines to local hospitals. Using drones may not usher in a technological revolution, but it reminds me of Carlota Perez&apos;s &lt;i&gt;Technological Revolutions and Financial Capital&lt;/i&gt;; in her model, technological revolutions generally start at certain cores (US, Britain, etc.), then diffuse to the peripheries. This appears to demonstrate the opposite phenomenon. Ghana&apos;s restrictions against drones may have been more lax, hence providing a nice training ground for Zipline.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.ft.com/content/3de60b3b-fa83-466a-b38d-d5cde3680b36&quot;&gt;Robinhood&lt;/a&gt; faced to pay $70 million to resolve allegations that it misled customers. The psychology surrounding money is fascinating; at least in America, having a lot of money is synonymous with a good life, and I think the behaviors seen on Robinhood are in-line with this thesis.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Wednesday, June 30, 2021&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/06/29/the-engineering-daring-that-led-to-the-first-chinese-personal-computer/&quot;&gt;Sinotype III&lt;/a&gt; and how Chinese characters arrived onto the digital computing scene. Sinotype III was an Apple II with a modified word processor and operating system; it was developed (strangely?) by the Graphic Arts Education and Research Foundation (GARF), an American foundation.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.reuters.com/business/chinas-didi-raises-4-billion-us-ipo-source-2021-06-29/&quot;&gt;Didi Chuxing&lt;/a&gt;, a Chinese ride-hailing company, to raise an expected $4 billion in US IPO. A lot of Didi&apos;s ride-sharing comes from taxi-hailing rather than private-car sharing (similar to Kakao Taxi), and with Meituan and Ele.me already popular in the Chinese food delivery market, it&apos;ll be interesting to see how Didi grows.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://techcrunch.com/2021/06/29/family-app-life360-announces-2-1m-investment-round-from-celebs-and-influencers/&quot;&gt;Life360&lt;/a&gt; announces a new investment round of $2.1 million. This trend of wanting to know the locations of your families and friends is an interesting one. I remember it started back in the 2010s with Snapchat announcing their locations feature. Since then, I&apos;ve had multiple friends ask me to install location-sharing apps, but I never understood why someone would willingly give away his/her location. Maybe the placement of a social graph in physical space?&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-06-29/sam-altman-s-worldcoin-will-give-free-crypto-for-eyeball-scans?srnd=technology-vp&quot;&gt;Worldcoin&lt;/a&gt;, a cryptocurrency that requires a retinal scan for identification. As the name suggests, it strives for universal basic income by bypassing government regulation with technology. The relationship between centralized and decentralized systems, and the analogous tension between centralized and decentralized &lt;b&gt;power&lt;/b&gt; comes to mind here.&lt;/li&gt;

	&lt;li&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2021-06-29/eighty-year-old-japanese-firm-may-be-key-to-next-gen-chip-tech&quot;&gt;Disco Corps&lt;/a&gt; is a Japanese company that specializes in precision grinding and dicing equipment, which are essential for semiconductor manufacturing.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Preface Louis Pasteur said &quot;where observation is concerned, chance favors only the prepared mind.&quot; For me, there are three components to intellectual growth: observation, intake, and reflection. Observation is by far the component that I pay the least attention to, and this page is an attempt to give more effort to observation.</summary></entry><entry><title type="html">Files</title><link href="http://localhost:3000/2021/06/12/files/" rel="alternate" type="text/html" title="Files" /><published>2021-06-12T00:00:00+09:00</published><updated>2021-06-12T00:00:00+09:00</updated><id>http://localhost:3000/2021/06/12/files</id><content type="html" xml:base="http://localhost:3000/2021/06/12/files/">&lt;h3&gt;Notes&lt;/h3&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STAT_244.pdf&quot;&gt;STAT 24400&lt;/a&gt; - Winter 2020, taught by Dr. Rina Barber&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/STAT245_cheatsheet.pdf&quot;&gt;STAT 24500&lt;/a&gt; - Spring 2021, taught by Dr. Chao Gao&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Misc.&lt;/h3&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;/assets/data/files/ergodic_entropy.pdf&quot;&gt;Equivalent Notions of Entropy Under Ergodicity&lt;/a&gt; - UChicago REU 2020&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Notes STAT 24400 - Winter 2020, taught by Dr. Rina Barber STAT 24500 - Spring 2021, taught by Dr. Chao Gao</summary></entry><entry><title type="html">Audio Fingerprinting</title><link href="http://localhost:3000/2021/01/03/fingerprint/" rel="alternate" type="text/html" title="Audio Fingerprinting" /><published>2021-01-03T00:00:00+09:00</published><updated>2021-01-03T00:00:00+09:00</updated><id>http://localhost:3000/2021/01/03/fingerprint</id><content type="html" xml:base="http://localhost:3000/2021/01/03/fingerprint/">My project: &lt;a href=&quot;https://github.com/alikiki/audioplay&quot;&gt;audioplay&lt;/a&gt;
&lt;h2&gt;Basic Signal Processing Basics&lt;/h2&gt;
	&lt;h4&gt;Waves&lt;/h4&gt;
		&lt;p&gt;When a note is played on the piano, the sound that emanates from the piano and reaches our ears is actually a wave. As soon as the piano hammer hits the string, the air surrounding the piano string vibrates i.e. areas of low air pressure and high air pressure are created. These vibrations travel throughout the air and causes our ear drums to vibrate, causing other ear components to send signals to the brain. These signals are interpreted as &quot;sound&quot;. [1]&lt;/p&gt;

		&lt;div class=&quot;marginnote&quot;&gt;[1] Not all waveforms look like the clean sine and cosine functions; any look at the stock market shows that waveforms can look extremely complicated.&lt;/div&gt;

	&lt;h4&gt;Fourier Transform&lt;/h4&gt;
		&lt;p&gt;Any waveform can be broken down into its constituent sine and cosine waves (also called simple sinusoids), for which the terms &lt;b&gt;frequency&lt;/b&gt; and &lt;b&gt;amplitude&lt;/b&gt; are more well-defined. For example, the waveform below is pretty irregular, so it&apos;s difficult to observe any concrete properties by just looking at it. Luckily, the Fourier Transform decomposes the waveform into simple sinusoids, from which we can observe the various frequencies that make up the waveform.&lt;/p&gt;
		&lt;img src=&quot;/assets/data/fingerprint/fourier.png&quot; style=&quot;display: block; width: 60%;margin-left: auto; margin-right: auto;&quot;&gt;


		&lt;div class=&quot;marginnote&quot;&gt;[2] The best of both time and frequency worlds is encapsulated in the spectrogram, which we introduce soon.&lt;/div&gt;
		&lt;p&gt;Notice how the faded orange, green, and pink waves are just simple sinusoids with uniform frequencies and waveforms. &lt;b&gt;This is huge.&lt;/b&gt; The Fourier Transform implies that waveforms can be represented in terms of both time and frequency, each representation showing what the other cannot. [2] The canonical representation of waves is time-based, but notice that the temporal representation offers no obvious frequency information. On the other hand, the frequency representation likewise eliminates time information.&lt;/p&gt;



&lt;h3&gt;Digitalization&lt;/h3&gt;
	&lt;p&gt;The above introduced the necessary tools and information for audio fingerprinting. However, music in its digital form places critical restrictions on these tools - as always, knowing when not to apply certain tools is just as important as knowing when to apply them, so we explore the relevant consequences of digitalization below.&lt;/p&gt;
	&lt;h4&gt;Sampling and its consequences&lt;/h4&gt;
		&lt;p&gt;The difference between analog and digital music may be slight for the casual music listener, but most audiophiles will acknowledge that analog music sounds warmer and deeper compared to its counterpart - this is due to the process in which analog music is converted to digital music, or sampling. Sampling transforms a continuous signal into a discrete one. [3] As a simple illustration, making a connect-the-dots puzzle from a line drawing is essentially sampling - a conversion from the continuous to the discrete.&lt;/p&gt;

		&lt;div class=&quot;marginnote&quot;&gt;[3] &quot;Discrete&quot; and &quot;continuous&quot; are just math jargon for &quot;chopped up&quot; and &quot;smooth&quot;, respectively.&lt;/div&gt;

		&lt;img src=&quot;/assets/data/fingerprint/sampling.png&quot;&gt;

		&lt;p&gt;The specific mechanics of sampling consists of marking dots on a continuous signal at a fixed speed (called the sampling rate). But how fast should the sampling rate be such that the continuous signal can be deduced from its discretized form? We should hope that it&apos;s fast enough so that the deduction is easier than completing a connect-the-dots puzzle. A slow sample rate keeps us guessing, but a fast sample rate clearly shows the shape of the continuous signal.&lt;/p&gt;

		&lt;img src=&quot;/assets/data/fingerprint/fast_slow.png&quot;&gt;

		&lt;p&gt;However, as the sample rate increases, the memory demand also increases. We&apos;re faced with a clear dilemma here: either increase the sample rate and risk taking up too much memory, or decrease the sample rate and lose data to the point that the original signal is impossible to deduce. Where is the Goldilock&apos;s zone of sampling rates for music?&lt;/p&gt;

		&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem&quot;&gt;The Nyquist-Shannon sampling theorem&lt;/a&gt; states that given a signal that contains no frequencies greater than \( B \) hertz, a sample rate of at least \( 2B \) samples per second is sufficient to reconstruct the continuous signal from its discretized form. Since human ears can only hear up to around 20 kHz, a sample rate of 40 kHz should be sufficient. This is where the magic number of 44.1 kHz comes from. &lt;/p&gt;

		&lt;p&gt;It seems as though we have resolved our digitalization problem. Sampling at 44.1 kHz is sufficient, so now we should be free to analyze digital waveforms. Unfortunately, the Fourier Transform in its current form poses a major obstruction.&lt;/p&gt;

	&lt;h4&gt;Discrete Fourier Transform (DFT)&lt;/h4&gt;

		&lt;p&gt;The Fourier transform can extract frequency information only from continuous waveforms. Luckily, a discrete version of the Fourier Transform exists, the aptly-named &lt;a href=&quot;https://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;&gt;Discrete Fourier Transform&lt;/a&gt;. Without going into too much technical detail, the DFT essentially bins frequency information. The frequencies that the DFT extracts are grouped together; the exact specifications of these groupings is determined by the sampling rate and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Window_function&quot;&gt;window&lt;/a&gt;. We skip the technical details in this layman&apos;s introduction.&lt;/p&gt;

&lt;h2&gt;audioplay: reverse-search&lt;/h2&gt;
	&lt;p&gt;As an exercise, let&apos;s explore what is important in song identification. As studied above, the essential components of any song are frequency, time, and amplitude. 

	&lt;ul&gt;
		&lt;li&gt;&lt;b&gt;Amplitude.&lt;/b&gt; Clearly, the loudness (a.k.a. amplitude) is irrelevant; a song played at 10 dB is still the same song when played at 100 dB. &lt;/li&gt;
		&lt;li&gt;&lt;b&gt;Frequency.&lt;/b&gt; A song pitched up several semitones is noticeably different from the original song, so frequency should be important. Of course, the original song will be recognizable, but to say that the pitched-up version is the same as the original would be a stretch.&lt;/li&gt;
		&lt;li&gt;&lt;b&gt;Time.&lt;/b&gt; Similarly, a sped-up song is not the same as the original. However, overall song length is generally useless; instead, the time differences between specific moments are meaningful identifiers. For example, if a note hits earlier than you expect, then you will probably second-guess your memory. &lt;/li&gt;
	&lt;/ul&gt;
	&lt;/p&gt;
	&lt;p&gt;With these ideas in mind, the project consists of two components: fingerprinting and recognition. The first transforms sound into a common medium that we can work with, and the second pertains to the song identification itself. In other words, fingerprinting is the mise en place, while recognition is the actual cooking.&lt;/p&gt;
	
	&lt;p&gt;The project&apos;s general overview is as follows:
	&lt;ol&gt;
		&lt;li&gt;Fingerprinting&lt;/li&gt;
			&lt;ul&gt;
				&lt;li&gt;Spectrogram generation&lt;/li&gt;
				&lt;li&gt;Peak detection&lt;/li&gt;
				&lt;li&gt;Peak association&lt;/li&gt;
				&lt;li&gt;Hashing&lt;/li&gt;
			&lt;/ul&gt;
		&lt;li&gt;Recognition&lt;/li&gt;
			&lt;ul&gt;
				&lt;li&gt;Fingerprinting&lt;/li&gt;
				&lt;li&gt;Scoring&lt;/li&gt;
			&lt;/ul&gt;
	&lt;/ol&gt;&lt;/p&gt;

&lt;h3&gt;Fingerprinting&lt;/h3&gt;
&lt;h4&gt;I. Spectrogram generation&lt;/h4&gt;
	&lt;p&gt;The spectrogram combines time and frequency information into a single representation, hence returning the time information lost through the Fourier transform. Because it compresses all the necessary information of a song into a single representation, it is the focal point of the entire fingerprinting process.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/spectro.jpg&quot; style=&quot;display: block; width: 80%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[4] Notice that the lower frequencies i.e. bass frequencies are especially high in amplitude. But when you listen to a song, the bass doesn&apos;t sound drastically louder than any of the other frequencies, so why are the bass frequencies so red? Because human ears are more sensitive to certain frequencies than others, sound engineers boost up the bass to make the overall track more &quot;even&quot;. For the curious, look up &quot;psychoacoustics&quot;.&lt;/div&gt;

	&lt;p&gt;The x-axis, y-axis, and the color show the time, frequency, and amplitude, respectively. Just for intuition, notice that the above spectrogram shows red spikes, where nearly all the frequencies in the entire range are firing off at a high amplitude (this is probably the snare of the song). [4]&lt;/p&gt;

	

&lt;h4&gt;II. Peak detection&lt;/h4&gt;
	&lt;p&gt;For our purposes, the spectrogram is an image, and an image is just an \(n \times n\) array of numbers, where each pixel is a cell in the array. The time and frequency determines the cell&apos;s location, and the amplitude dictates its value. A peak is then a (time, frequency) pair with the greatest amplitude value among the pixels surrounding it.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/spec_draw1.png&quot; style=&quot;display: block; width: 80%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;p&gt;To identify peaks, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_morphology&quot;&gt; binary morphology&lt;/a&gt;. [5] The process is as follows:
		&lt;div class=&quot;marginnote&quot;&gt;[5] Binary morphology has a lot of applications. Your drawing program probably uses binary morphology to make lines thinner or thicker, for example.&lt;/div&gt;
		&lt;ol&gt;
			&lt;li&gt;Place a 19-by-19 pixel square \(S\) over any part of the image.&lt;/li&gt;
			&lt;li&gt;Identify the pixel with the maximum value within \(S\). Call this maximum value \(M\).&lt;/li&gt;
			&lt;li&gt;Set all the pixel values within \(S\) to \(M\).&lt;/li&gt;
			&lt;li&gt;Apply the following rule to every pixel in \(S\): for any pixel \(P\) in \(S\),
				&lt;ul&gt;
					&lt;li&gt;If \(P = M\), color the pixel white.&lt;/li&gt;
					&lt;li&gt;If \(P \neq M\), color the pixel black.&lt;/li&gt;
				&lt;/ul&gt;&lt;/li&gt;
		&lt;/ol&gt;&lt;/p&gt;



	&lt;p&gt;For example, executing steps 1 through 3 gives the &quot;Mask Applied&quot; image [6], and the last step produces the &quot;Identified Peaks&quot; image. In the &quot;Mask Applied&quot; image, there are distinct squares, which are the 19-by-19 pixel squares described above. The &quot;Identified Peaks&quot; image shows scattered white dots on a black background, where the white dots are the maximal value pixels.&lt;/p&gt;

		&lt;img src=&quot;/assets/data/fingerprint/morphology.png&quot;&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[6] Values in this grayscale image range from 0 to 255, with 0 being black and 255 being white.&lt;/div&gt;
	&lt;p&gt;Finding the peaks is then just a matter of gathering the locations i.e. (time, frequency) pairs of the white dots. You may notice that by identifying peaks, we&apos;ve lost all amplitude information, since the white dots show only the locations of the peaks. Thankfully, amplitude is not important for our purposes. &lt;/p&gt;

&lt;h4&gt;III. Peak Association&lt;/h4&gt;
	&lt;p&gt;Surprisingly, this list of peak locations is actually sufficient for our goal to &quot;reverse-search&quot; audio. But we are greedy. Search needs to be fast, and using the vanilla list of peak locations would take absurdly long. By associating peaks with each other, we get higher information density and easy searchability at the expense of more computation. Not a bad deal.&lt;/p&gt;

	&lt;p&gt;The intuition behind peak association follows from the principle that the number of links between things increases faster than the number of things themselves. For example, there is only 1 possible link between 2 things, but 45 possible links between just 10 things.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/scale.png&quot; style=&quot;display: block; width: 100%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;p&gt;This means that from just a list of 10 peaks, we can extract 4 times as much information. The bang for our buck increases quadratically too, so more peaks means higher information density.&lt;/p&gt;

	&lt;h5&gt;III-a. Anchors + Target Zones&lt;/h5&gt;
		&lt;p&gt;However, chaotically pairing all possible peaks won&apos;t be fruitful - we need order in the chaos to extract the most information possible. The pairing method is as follows:
			&lt;ol&gt;
				&lt;li&gt;Order the peaks by time.&lt;/li&gt;
				&lt;li&gt;Choose a peak, and call it the anchor \(A\).&lt;/li&gt;
				&lt;li&gt;Choose the target peaks by selecting the 10 consecutive peaks \(\{ T_1, ..., T_{10} \}\) that come after skipping the first 3 peaks that are immediately after \(A\). For example, if \(A\) is the 10th peak, then the target peaks are the 14th, 15th, ..., 23rd peaks.&lt;/li&gt;
				&lt;li&gt;Form the pair associations \( \{ (A, T_1), ..., (A, T_{10})\}. \)&lt;/li&gt;
				&lt;li&gt;Repeat by setting every peak as the anchor, insofar as the 10 target peaks exist. For example, if only 10 peaks exist, then the 10th peak cannot be an anchor peak because there are no more potential target peaks.&lt;/li&gt;
			&lt;/ol&gt;	
		&lt;/p&gt;
		&lt;img src=&quot;/assets/data/fingerprint/peak_assoc.png&quot; style=&quot;display: block; width: 75%;margin-left: auto; margin-right: auto;&quot;&gt;

		&lt;p&gt;The anchor point brings order to the chaos. Because every peak in the target zone is linked to a common anchor point, every target peak has a &quot;common denominator&quot;. In terms of ease of information access, there is a stark difference between saying &quot;John is friends with Anne, Bob, and Clark&quot;, versus &quot;Anne is friends with Bob, Clark is friends with John, etc&quot;.&lt;/p&gt;

&lt;h4&gt;IV. Hashing&lt;/h4&gt;
	&lt;p&gt;Unfortunately, this list of peak pairs isn&apos;t easily searchable, so we need a method to compress each pair into an easily searchable form. Hashing associates each pair with a unique string, akin to how license plates uniquely identify cars. Input an anchor-target pair into the hashing function, and out comes a string that uniquely identifies that pair.&lt;/p&gt;

	&lt;p&gt;Recall that we hypothesized that only time difference and frequency are essential for identification. While the specifics of the hashing function aren&apos;t important, what we input into our hashing function reflects our hypothesis.&lt;/p&gt;

	&lt;img src=&quot;/assets/data/fingerprint/hash.png&quot; style=&quot;display: block; width: 70%;margin-left: auto; margin-right: auto;&quot;&gt;

	&lt;p&gt;Remembering that every peak is a (time, frequency) point, for every anchor-target pair \( (A, T) \), identify:
		&lt;ul&gt;
			&lt;li&gt;\(F_{\text{anchor}} := \) frequency of anchor peak.&lt;/li&gt;
			&lt;li&gt;\(F_{\text{target}} := \) frequency of target peak.&lt;/li&gt;
			&lt;li&gt;\(T_{\text{offset}} := \) time difference between target peak and anchor peak.&lt;/li&gt;
		&lt;/ul&gt;	

	The triple \( (F_{\text{anchor}}, F_{\text{target}}, T_{\text{offset}}) \) becomes the input. By hashing every anchor-target pair, we generate a list of strings that uniquely identifies the song and is easily searchable. Having effectively translated music into organized, searchable data, this concludes the fingerprinting process.&lt;/p&gt;

&lt;h3&gt;Recognition&lt;/h3&gt;
	&lt;p&gt;The sous chef has mise en place-d all the ingredients, and now it&apos;s time to cook. For the rest of this section, assume that we have a database full of song fingerprints. Also, we refer to the audio clip that needs to identified as the &quot;id clip&quot;, and the database songs as the &quot;db clips&quot;. For example, we need to check if the id clip matches any of the db clips.&lt;/p&gt;
&lt;h4&gt;I. Fingerprinting&lt;/h4&gt;
	&lt;p&gt;Fingerprinting translates sound into a standardized form of data, so an audio clip must first be fingerprinted before recognition so that the clip can be analyzed. Fortunately, the same fingerprinting algorithms can be used for this stage, so nothing extra needs to be added.&lt;/p&gt;

&lt;h4&gt;II. Scoring&lt;/h4&gt;
	&lt;p&gt;As an exercise, let&apos;s study how similar the fingerprints generated from two different sources playing the same song are. Both sources should generate similar spectrograms, and hence similar peaks. Therefore, the anchor-target pairs should coincide, with the only possible dissimilarities being in time and amplitude. Nonetheless, the time &lt;b&gt;differences&lt;/b&gt; between the anchor-target pairs in both should be constant. In sum, the invariant information consists of the anchor-target pair frequencies and time differences, which are exactly the inputs of the hashing function.&lt;/p&gt;

	&lt;div class=&quot;marginnote&quot;&gt;[7] It&apos;s unlikely that the clip for identification is the exact same clip that was used to fingerprint the song for the database. Hence, we need to work probabilistically: which song in the database has the highest probability of being the song in the clip?&lt;/div&gt;

	&lt;p&gt;Therefore, if the song in an audio clip matches a song in the database, then the hashes must coincide. Then identification simply becomes a matter of choosing the song that has the highest number of hash matches, and we are done! [7]&lt;/p&gt;
	
&lt;h3&gt;References and Further Reading&lt;/h3&gt;
	&lt;ol&gt;
		&lt;li&gt;The &lt;a href=&quot;/assets/data/fingerprint/shazam.pdf&quot;&gt;original Shazam paper.&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Will Drevo&apos;s &lt;a href=&quot;https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/&quot;&gt;dejavu&lt;/a&gt; project was invaluable and I heavily borrowed from the project source code.&lt;/li&gt;
		&lt;li&gt;Cameron Macleod&apos;s &lt;a href=&quot;https://github.com/notexactlyawe/abracadabra&quot;&gt;abracadabra&lt;/a&gt; also helped a lot.&lt;/li&gt;
	&lt;/ol&gt;</content><author><name></name></author><summary type="html">My project: audioplay Basic Signal Processing Basics Waves When a note is played on the piano, the sound that emanates from the piano and reaches our ears is actually a wave. As soon as the piano hammer hits the string, the air surrounding the piano string vibrates i.e. areas of low air pressure and high air pressure are created. These vibrations travel throughout the air and causes our ear drums to vibrate, causing other ear components to send signals to the brain. These signals are interpreted as &quot;sound&quot;. [1]</summary></entry></feed>